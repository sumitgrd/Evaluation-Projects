{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84546a6b",
   "metadata": {},
   "source": [
    "HR Analytics Project- Understanding the Attrition in HR\n",
    "Project Description\n",
    "Every year a lot of companies hire a number of employees. The companies invest time and money in training those employees, not just this but there are training programs within the companies for their existing employees as well. The aim of these programs is to increase the effectiveness of their employees. But where HR Analytics fit in this? and is it just about improving the performance of employees?\n",
    "HR Analytics\n",
    "Human resource analytics (HR analytics) is an area in the field of analytics that refers to applying analytic processes to the human resource department of an organization in the hope of improving employee performance and therefore getting a better return on investment. HR analytics does not just deal with gathering data on employee efficiency. Instead, it aims to provide insight into each process by gathering data and then using it to make relevant decisions about how to improve these processes.\n",
    "Attrition in HR\n",
    "Attrition in human resources refers to the gradual loss of employees overtime. In general, relatively high attrition is problematic for companies. HR professionals often assume a leadership role in designing company compensation programs, work culture, and motivation systems that help the organization retain top employees.\n",
    "How does Attrition affect companies? and how does HR Analytics help in analyzing attrition? We will discuss the first question here and for the second question, we will write the code and try to understand the process step by step.\n",
    "Attrition affecting Companies\n",
    "A major problem in high employee attrition is its cost to an organization. Job postings, hiring processes, paperwork, and new hire training are some of the common expenses of losing employees and replacing them. Additionally, regular employee turnover prohibits your organization from increasing its collective knowledge base and experience over time. This is especially concerning if your business is customer-facing, as customers often prefer to interact with familiar people. Errors and issues are more likely if you constantly have new workers.\n",
    "\n",
    "\n",
    "Dataset Link-\n",
    "â€¢\thttps://github.com/FlipRoboTechnologies/ML_-Datasets/blob/main/HR%20Analytics/ibm-hr-analytics-employee-attrition-performance.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137085f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0778cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./HRPrediction/WA_Fn-UseC_-HR-Employee-Attrition.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b323bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df22f3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot('Attrition', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218d3b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23a7818",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ce04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('EmployeeCount', axis=1) # removing EmployeeCount from numerical columns\n",
    "df = df.drop('StandardHours', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accc1e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [feature for feature in df.columns if df[feature].dtype=='object']\n",
    "df[categorical_cols].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009388a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['BusinessTravel',\n",
    " 'Department',\n",
    " 'EducationField',\n",
    " 'Gender',\n",
    " 'JobRole',\n",
    " 'MaritalStatus',\n",
    " 'Over18',\n",
    " 'OverTime']\n",
    "\n",
    "df = pd.get_dummies(df, columns=cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a276bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Attrition', axis=1)\n",
    "y = df['Attrition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdf3bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a87871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Age',\n",
    " 'DailyRate',\n",
    " 'DistanceFromHome',\n",
    " 'Education',\n",
    " 'EmployeeNumber',\n",
    " 'EnvironmentSatisfaction',\n",
    " 'HourlyRate',\n",
    " 'JobInvolvement',\n",
    " 'JobLevel',\n",
    " 'JobSatisfaction',\n",
    " 'MonthlyIncome',\n",
    " 'MonthlyRate',\n",
    " 'NumCompaniesWorked',\n",
    " 'PercentSalaryHike',\n",
    " 'PerformanceRating',\n",
    " 'RelationshipSatisfaction',\n",
    " 'StockOptionLevel',\n",
    " 'TotalWorkingYears',\n",
    " 'TrainingTimesLastYear',\n",
    " 'WorkLifeBalance',\n",
    " 'YearsAtCompany',\n",
    " 'YearsInCurrentRole',\n",
    " 'YearsSinceLastPromotion',\n",
    " 'YearsWithCurrManager']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "for col in cols:\n",
    "    X_train[col] = scaler.fit_transform(X_train[col].values.reshape(-1 ,1))\n",
    "    X_test[col] = scaler.fit_transform(X_test[col].values.reshape(-1 ,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe177e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy = LazyClassifier()\n",
    "model, pred = lazy.fit(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d69f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sort_values('Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb099fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "neighbors = []  \n",
    "cv_scores = []  \n",
    "    \n",
    "from sklearn.model_selection import cross_val_score  \n",
    "# perform 10 fold cross validation  \n",
    "for k in range(1, 40, 2):  \n",
    "    neighbors.append(k)  \n",
    "    knn = KNeighborsClassifier(n_neighbors = k)  \n",
    "    scores = cross_val_score(  \n",
    "        knn, X_train, y_train, cv = 10, scoring = 'accuracy')  \n",
    "    cv_scores.append(scores.mean()) \n",
    "error_rate = [1-x for x in cv_scores]  \n",
    "    \n",
    "# determining the best k  \n",
    "optimal_k = neighbors[error_rate.index(min(error_rate))]  \n",
    "print('The optimal number of neighbors is % d ' % optimal_k)  \n",
    "    \n",
    "# plot misclassification error versus k  \n",
    "plt.figure(figsize = (10, 6))  \n",
    "plt.plot(range(1, 40, 2), error_rate, color ='blue', linestyle ='dashed', marker ='o', \n",
    "         markerfacecolor ='red', markersize = 10) \n",
    "plt.xlabel('Number of neighbors')  \n",
    "plt.ylabel('Misclassification Error')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310910a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict, cross_val_score \n",
    "from sklearn.metrics import accuracy_score, classification_report \n",
    "from sklearn.metrics import confusion_matrix \n",
    "  \n",
    "def print_score(clf, X_train, y_train, X_test, y_test, train = True): \n",
    "    if train: \n",
    "        print(\"Train Result:\") \n",
    "        print(\"------------\") \n",
    "        print(\"Classification Report: \\n {}\\n\".format(classification_report( \n",
    "                y_train, clf.predict(X_train)))) \n",
    "        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix( \n",
    "                y_train, clf.predict(X_train)))) \n",
    "  \n",
    "        res = cross_val_score(clf, X_train, y_train,  \n",
    "                              cv = 10, scoring ='accuracy') \n",
    "        print(\"Average Accuracy: \\t {0:.4f}\".format(np.mean(res))) \n",
    "        print(\"Accuracy SD: \\t\\t {0:.4f}\".format(np.std(res))) \n",
    "        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score( \n",
    "                y_train, clf.predict(X_train)))) \n",
    "        print(\"----------------------------------------------------------\") \n",
    "                 \n",
    "    elif train == False: \n",
    "        print(\"Test Result:\") \n",
    "        print(\"-----------\") \n",
    "        print(\"Classification Report: \\n {}\\n\".format( \n",
    "                classification_report(y_test, clf.predict(X_test)))) \n",
    "        print(\"Confusion Matrix: \\n {}\\n\".format( \n",
    "                confusion_matrix(y_test, clf.predict(X_test))))  \n",
    "        print(\"accuracy score: {0:.4f}\\n\".format( \n",
    "                accuracy_score(y_test, clf.predict(X_test)))) \n",
    "        print(\"-----------------------------------------------------------\") \n",
    "          \n",
    "knn = KNeighborsClassifier(n_neighbors = 7) \n",
    "knn.fit(X_train, y_train) \n",
    "print_score(knn, X_train, y_train, X_test, y_test, train = True) \n",
    "print_score(knn, X_train, y_train, X_test, y_test, train = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe52444",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0   # We set our random seed to zero for reproducibility\n",
    "# Random Forest parameters\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 1000,\n",
    "#     'warm_start': True, \n",
    "    'max_features': 0.3,\n",
    "    'max_depth': 4,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'random_state' : seed,\n",
    "    'verbose': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff3b7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(**rf_params) \n",
    "rfc.fit(X_train, y_train) \n",
    "print_score(rfc, X_train, y_train, X_test, y_test, train = True) \n",
    "print_score(rfc, X_train, y_train, X_test, y_test, train = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206ed504",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_params ={\n",
    "    'n_estimators': 1500,\n",
    "    'max_features': 0.9,\n",
    "    'learning_rate' : 0.25,\n",
    "    'max_depth': 4,\n",
    "    'min_samples_leaf': 2,\n",
    "    'subsample': 1,\n",
    "    'max_features' : 'sqrt',\n",
    "    'random_state' : seed,\n",
    "    'verbose': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d3c4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(**gb_params) \n",
    "gbc.fit(X_train, y_train) \n",
    "print_score(gbc, X_train, y_train, X_test, y_test, train = True) \n",
    "print_score(gbc, X_train, y_train, X_test, y_test, train = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ffe05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = rfc.feature_importances_\n",
    "feature_importances = pd.Series(feature, index=X_train.columns).sort_values(ascending = False)\n",
    "sns.barplot(x=feature_importances[0:10], y=feature_importances.index[0:10])\n",
    "sns.despine()\n",
    "plt.xlabel(\"Feature Importances\")\n",
    "plt.ylabel(\"Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694440d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
